---
title: "Storage and internals"
description: ""
date: "2026-02-05"
---



Mongodb is primarily a document store.A mongodb database can span across multiple nodes and continents. A database can contain multiple collections(like table) which is again group of documents. Each document is just a json object. The internal engine of mongodb is called `WiredTiger`. 

The mongodb is a distributed database which can work on multiple nodes. 

The mongodb software has three parts - 
- mongod - It is the deamon process which is responsible for all the important tasks like storage , i/o and handling. On each storage node of mongodb server this mongod runs. In a simple setup, you run one `mongod`. In a production setup, you run multiple `mongod` instances in a **Replica Set** to ensure high availability.
- mongos - Its the router. Mongodb can run in both sharded and replicated form. In full fledged running mongodb runs with multiple replication clusters. In a repllicated cluster all the nodes hold the same data for high availability. However there is one writer to which first write happens. While in sharded format each replication cluster (multinode/single node) has only the subset of data. Router acts as middlelayer redirecting the request to the correct cluster/mongod. 
- mongosh - It is a JavaScript-based terminal that allows you to type commands.

## JSON and bson

While you interact with MongoDB using JSON-like documents, the database "thinks" and stores data in BSON. JSON thus acts as primary abstraction between storage and applcation. Json has very limited data types - Strings, numbers , Null ,Arrays and Boolean. BSON is the **binary-encoded serialization** of JSON-like documents. MongoDB uses BSON as the primary data format for storage and network transfer. 

BSON has richer data types Int32 , Date , Timestamp etc. Note that json data never reaches mongod. Rather its the application driver which converts the json into bson before sending and also converts back when receiving. 

#### BSON formatting

Unlike json which uses curly braces `{}` and colon `:` to define the structure. BSON uses length prefix, type tagged binary format. Every BSON document starts with a 4-byte little-endian integer representing the total size of the document.

This is followed by a sequence of elements, and finally, a null terminator byte (`0x00`) to signal the end of the object. This structure allows MongoDB to jump to the end of a document or skip specific fields without having to parse every single character, which is a massive performance win over standard JSON.

Each field in bson is composed of three distinct parts - 

- type indicator - It is one byte and tells what is the type of field. Eg - `0x02` for strings.
- field name - The name of key stored as null terminated strings. 
- value - The actual data, formatted according to its type. For a string, this includes its own length prefix; for an integer, it’s a fixed-width binary representation.

If mongo would have stored in json format since it is text based, it turns out for each number we would have to convert the strings into the numbers do the compute and again change it to string format for storage. With bson numbers are autoformatted to get stored into the type for exmaple Int32. This means the number can be directly computed. 

Because BSON stores the length of elements (like strings, sub-documents, and arrays) at the very beginning of that element, it enables **linear scanning optimization**. If you are looking for a field at the end of a large document, the BSON parser reads the length of the current field and "jumps" its pointer forward by that many bytes to the next field.

In contrast, a JSON parser must inspect every single character (looking for commas, quotes, and braces) to find where one field ends and the next begins. This "skip-ahead" capability is why MongoDB can handle massive documents with high throughput.
### Object id

Each document has an `_id` field. This field is autoindexed and must be unique. If the user does not provide `_id` while inserting. It is autogenerated. Stored as 12 bytes it has three parts - 

- 4 bytes - Timestamp
- 5 bytes - Random value
- 3 bytes - Increment counter

### Arrays vs subdocuments 

A sub-document is simply a BSON document embedded within the value field of another BSON document. It follows the exact same rules: a 4-byte length prefix, a sequence of elements (Type + Key + Value), and a null terminator. When the database engine encounters a sub-document type (`0x03`), it treats the "Value" portion as a completely new, self-contained BSON stream.

This recursive structure allows MongoDB to support nesting up to 100 levels deep, though performance usually degrades long before that.

Mongodb stores arrays as the objects. In arrays the keys are the string representation of indices. Because these indices are stored as strings (C-strings), an array in BSON actually carries more overhead than a sub-document with short keys. When you update a specific element in an array for example, `tags.2`—MongoDB doesn't have to parse the whole array. It uses the **length prefixes** of the preceding elements to "jump" through the byte stream until it finds the key `"2"`.

### Compression 

The internal engine wiredtiger usually has to keep the bson documents in memory. However before it flushes to storage it compresses. There are three diffreent algos which mongodb provides for compression. Understand that higher compression you do less storage space it takes but the more cpu computation is needed so in case the disk is hitted for a db query much more cpu and slower will be the response. `Snappy` is the dafault on mongodb which provides least compression but uses very little cpu. Others take more cpu but do better compression. 

### Anatomy of write operation

Mongodb proides the `Write ahead logging(WAL)` strategy via journal. This ensures that even if power is cut of, data is lost. 

1. First application sends a write command. Driver converts your native object into bson and sends it over the wire. 
2. When the call reaches the mongod node(Note we have ommited the part if it was running as replicated db) write is recoreded to journal. . This is a sequential, compressed log on disk. If the server crashes, MongoDB replays this log to recover.
3. WiredTiger cache: Simultaneously document is written to the internal cache. At this stage, the write is "complete" from the perspective of the application (depending on your Write Concern). The data is now in RAM but not yet in the main `.wt` data files.
4. **The Oplog (Replication):** If you are in a Replica Set, the write is added to the `oplog.rs` (Operations Log). Secondary nodes pull from this log to stay in sync.
5. Every 60 seconds (or when 2GB of data is written), WiredTiger performs a **Checkpoint**. It takes all the "dirty" pages in the cache, compresses them (Snappy/Zstd), and flushes them to the final `.wt` files on disk.

#### Anatomy of read operation

WiredTiger tries to avoid the disk read all costs. 

1. Query parsing is done first. Then the query plan is decided. 
2. Internal chache lookup is done if found well and good.
3. Otherwise page fault has happended and wiredtiger will find the pages on the disk take them out decompresses them repopulates the cache and returns result. 

### Locking

Mongodb does different kinds of locking in different cases. For admin tasks mostly entire db will be locked. Otherwise MongoDB uses intent locks. Intent locks are done to the higher levels so that it does not do something catastrophic when lover level is doing something. For example say some `document` is getting modified. `Mongodb` places `intent reading` on the collection and db. This is done so that other user does not deletes the entire db when update is being done. 

These are called lock modes and 4 exists

| **Mode** | **Name**         | **Description**                                                               |
| -------- | ---------------- | ----------------------------------------------------------------------------- |
| **S**    | Shared           | **Read lock.** Multiple readers can hold this at once.                        |
| **X**    | Exclusive        | **Write lock.** Only one writer can hold this; no one else can read or write. |
| **IS**   | Intent Shared    | Indicates a lock is held at a lower level for **reading**.                    |
| **IX**   | Intent Exclusive | Indicates a lock is held at a lower level for **writing**.                    |
With these only some operations will be blocked and locking is as granular as possible. Wiredtiger uses optimistic concurrency control (OCC). OCC is good when contention is low meaning there is very small chance that two different threads are trying to update same document. 

Following is done while write - 

- Move document into cache
- Apply changes in memory
- Checks if anyone else modified in mean time
- If yes then it triggeres write conflict and operation is automatically retried. 

This OCC is something which gives good write throughput to mongodb. 

Common operations and locks-

- Find - Intent Shared (IS) on Database/Collection, Shared (S) on Document.
- **Insert/Update/Delete:** Intent Exclusive (IX) on Database/Collection, Exclusive (X) on Document.
- **Create Index:** Prior to 4.2, this locked the collection. Now, indexes are built "build-in-background" style, taking only a very brief lock at the start and end.
- **Drop Collection:** Exclusive (X) lock on the entire Database. Nothing else can happen in that DB until the drop is finished. 

### Indexing

Indexing is the single most important factor for performance in MongoDB. Without an index, MongoDB must perform a **Collection Scan** (COLLSCAN), meaning it has to read every single document in a collection to find the ones that match your query. 

MongoDB uses **B-Trees** (Balanced Trees) for its indexes. When you index a field, MongoDB creates a tree structure where each "node" contains a range of values and a pointer to the actual BSON document on disk (the Record ID).

**Sorted Order:** The B-Tree keeps keys in a specific order (ascending or descending).

| **Index Type**         | **Use Case**                                                                                |
| ---------------------- | ------------------------------------------------------------------------------------------- |
| **Single Field**       | A basic index on one field (e.g., `user_id`).                                               |
| **Compound**           | An index on multiple fields (e.g., `{ "lastName": 1, "firstName": 1 }`). Order matters!     |
| **Multikey**           | Used for **Arrays**. MongoDB creates an index entry for _every_ element in the array.       |
| **TTL (Time To Live)** | Automatically deletes documents after a certain amount of time. Great for logs or sessions. |
| **Text**               | Supports search for words and phrases within string content.                                |
| **Geospatial**         | For "near" queries and coordinate-based data (2dsphere).                                    |
| **Hashed**             | Maps values to a hash for even distribution, often used in **Sharding**.                    |
Compound indexes are powerful but tricky. The order of fields in a compound index determines its usefulness. This is governed by the **ESR Rule**: **E**quality, **S**ort, **R**ange.If you have an index on `{ "status": 1, "age": 1 }`: Then directly querying `{"age":100}` will be as slow as entire db scan. 

A **Covered Query** is the "holy grail" of performance. It happens when:

- All the fields in the query are part of the index.
- All the fields returned (the projection) are also in the index.

### Distributed setting

In a distributed environment, MongoDB moves from being a single data store to a sophisticated cluster of machines working in tandem. This setup is built on two distinct pillars: **Replication** for high availability and **Sharding** for horizontal scalability.

Replication is about redundancy. A **Replica Set** is a group of `mongod` instances that maintain the same data set. This ensures that if one server goes up in smoke, your application stays online.

- **The Primary Node:** Only one node in a set is the "Primary." It is the only node that can accept writes. It records every change to its data in a special capped collection called the **oplog** (operations log).

- **Secondary Nodes:** These nodes continuously poll the Primary’s oplog and apply those same operations to their own data sets asynchronously.

- **Heartbeats & Elections:** Every node pings the others every 2 seconds. If the Primary stops responding for more than 10 seconds, the Secondaries hold an automatic **election** to choose a new Primary. This failover usually happens in under 12 seconds.


While replication handles availability, Sharding handles **volume**. When your data exceeds the storage or CPU capacity of a single machine, you "shard" it—splitting the collection into smaller pieces and distributing them across multiple Replica Sets.

A Sharded Cluster consists of three core components:

-  **Shards:** Each shard is a Replica Set that holds a subset of the total data.
- **Config Servers:** A separate, small Replica Set that stores the **metadata**—the "map" of which data lives on which shard.
- **Mongos (The Router):** This is a lightweight, stateless process that acts as the entry point for your application. You connect to `mongos`, and it uses the config server's map to route your query to the correct shard.

While sharding can be it is important to choose shard key wisely. This is the field (or fields) MongoDB uses to decide how to partition your documents. Many things are important while sharding - Like query pattern , what kind of data are you frequently quering , write pattern. We want to have sharding so that single read is from one sharded cluster. This keeps fanout low and so extra network calls are low. Similarly for writes we want to have writes happening in as distributed manner as possible. For example having the timestamp as one shardkey will be wrong as all the writes at a given point will be going over one sharded instance only. Meaning all the write updates will be happening on a single node. Reducing speed. Better could have been hashed timestamp. 

### Consistency vs speed

Now consider one replacted cluster. It has one primary write node and secondry read nodes. Now we can reduce the consistency(CAP theorum) by availability using write concern(`w`). 

This defines how many nodes must acknowledge a write before the driver tells your app "Success."

- `w: 1`: Only the Primary acknowledged (Fast, but riskier).
- `w: "majority"`: The majority of nodes in the set (e.g., 2 out of 3) acknowledged. This is the "Gold Standard" for preventing data loss during a failover.

Read preference - 

This defines _where_ the data comes from.

- **Primary:** (Default) Always get the freshest data.
- **Secondary:** Read from a secondary to reduce the load on the Primary. **Beware:** Because replication is asynchronous, you might read "stale" data (data that hasn't arrived at the secondary yet).

In a production-grade distributed setup, your architecture looks like this:
- Your application connects to a **Load Balancer** sitting in front of multiple **Mongos** routers.
- The **Mongos** routers talk to a **Config Server Replica Set**.
- The data itself is spread across multiple **Shard Replica Sets**.

Finally each domument has `16mb` limit. 

#### ACID transactions

Now that one document is locked before oding any other operation. One doc update is always acid complient. Since `Mongo4.0`  we get multidoc acid transations. It uses a **global logical clock** and a "snapshot" isolation level. You should know that while they exist, they carry a performance penalty and should not be used as a "crutch" for poor schema design.


### Database patterns for NoSQL

NoSQL data modeling shifts the focus from rigid table structures to the specific access patterns of the application. Unlike relational databases that normalize data to reduce redundancy, NoSQL often embraces denormalization to optimize for read speed and horizontal scalability.

#### The Outlier Pattern

The **Outlier Pattern** is a strategic way to handle "corner cases" where a handful of documents deviate significantly from the average data size or relationship count. In document databases like MongoDB, there is often a physical limit on document size (e.g., 16MB). If you are modeling a social media platform, most users might have a few hundred followers, which fits easily into an array within the user document. However, for a celebrity with 50 million followers, that array would explode and crash the system.

Instead of forcing a "one size fits all" schema, you store the bulk of the data in the main document but add a flag like `has_overflow: true`. The additional data is then moved to a separate collection or "overflow" documents. This keeps the database performant for 99% of your users while gracefully handling the "outliers" without hitting architectural ceilings.

#### The Extended Reference Pattern

The **Extended Reference Pattern** is the NoSQL answer to the performance tax of joins. In a traditional setup, to display an order history, you might need to join an `Orders` table with a `Products` table. In a high-traffic environment, these joins are expensive. With Extended Reference, you identify the specific fields that are most frequently accessed together—such as a product's name and its current price—and embed _only_ those fields directly into the order document.

For example, an `Order` document wouldn't just store a `product_id`; it would also store the `product_name` and `unit_price` at the time of purchase. While this creates data redundancy, it ensures that your most common "read" operation (viewing an invoice) requires only a single database hit. You aren't duplicating the entire product catalog; you are just "extending" the reference to include the vital metadata needed for the UI.

#### Attribute pattern 

The **Attribute Pattern** is designed to solve the "big table" problem where documents have hundreds of potential fields, but each individual document only uses a few of them. This is common in e-commerce catalogs where a "Laptop" has a CPU and RAM, but a "Water Bottle" has volume and material. Creating a unique index for every possible attribute would be an indexing nightmare.

Instead of hundreds of sparse fields, you use an array of key-value pairs, such as `attributes: [{ "k": "color", "v": "blue" }, { "k": "material", "v": "steel" }]`. This allows you to create a single, efficient index on the `k` and `v` fields. It makes your data highly searchable and flexible, as you can add new product specifications on the fly without ever needing to perform a schema migration or add new indexes to the database.








