---
title: "6. Concurrency and transactions"
description: ""
date: "2026-02-05"
---



Concurrency arises when multiple users or applications try to access the database at the same time.  When concurrent transactions take place without concurrency control following can happen - 

- **Dirty Reads:** Transaction A reads data that Transaction B changed but hasn't committed yet. If Transaction B rolls back, Transaction A is left with "fake" data.
- **Non-Repeatable Reads:** Transaction A reads a row. Transaction B updates that same row and commits. If Transaction A reads the row again, it sees a different value. 
- **Phantom Reads:** Transaction A queries a range of rows (e.g., "all users in NY"). Transaction B inserts a new user in NY. If Transaction A runs the same query again, a new "phantom" row appears.
- **Lost Updates:** Two transactions read the same value, modify it, and write it back. The last one to write simply overwrites the first one's changes without knowing it.

Transaction is a group of `SQL` statements in the loose sense. ACID compliance means the following - 

### ACID properties

One of PostgreSQL's greatest strengths is its strict adherence to ACID (Atomicity, Consistency, Isolation, and Durability) properties. If you've used databases like MongoDB or Cassandra, you're familiar with eventual consistency or relaxed transaction guarantees which are common trade-offs in NoSQL databases. PostgreSQL takes a different approach – it ensures that your data always follows all defined rules and constraints (like foreign keys, unique constraints, and custom checks), and that all transactions complete fully or not at all, even if it means sacrificing some performance.

#### Atomicity

```sql
BEGIN;
  UPDATE accounts SET balance = balance - 100 WHERE account_id = 'savings';
  UPDATE accounts SET balance = balance + 100 WHERE account_id = 'checking';
COMMIT;
```

Atomicity guarantees that either both operations succeed or neither does. If the system crashes after deducting from savings but before adding to checking, PostgreSQL will roll back the entire transaction. Your money never disappears into thin air.

#### Consistency

Consistency ensures that transactions can only bring the database from one valid state to another.

```sql
CREATE TABLE accounts (
    account_id TEXT PRIMARY KEY,
    balance DECIMAL CHECK (balance >= 0),
    owner_id INTEGER REFERENCES users(id)
);
```

If a transaction would make your balance negative, PostgreSQL will reject the entire transaction.

Confusingly, consistency in ACID has a slightly different meaning than consistency in CAP Theorem. In ACID, consistency means that the database always follows all defined rules and constraints. In the CAP Theorem, consistency means that the database always returns the correct result, even if it means sacrificing availability or partition tolerance.

#### Isolation

_Isolation_ means that each transaction runs **as if it were the only transaction** executing on the system — even though many might actually run at the same time.

It prevents **interference** between concurrent transactions, so one transaction’s intermediate (uncommitted) data isn’t visible to another.

Without isolation, two transactions running together could **read or modify each other’s partial results**, leading to problems like:

- Incorrect totals
- Missing updates
- Inconsistent reads

In reality there are isolation levels which define when multiple transations run concurrently how much others are allowed to see each other transation.

When multiple transactions run **concurrently**, the database needs to decide how much they are allowed to “see” or “interfere” with each other.

**Higher isolation = safer, but slower.**
**Lower isolation = faster, but may cause inconsistent results.**

#### Read uncommited 

Transactions can read data that has not been commited by others. No locks on reads. Its lowest safety but fastest.
There can be many problems due to this 

1. Dirty read: A dirty read happens when **one transaction reads data that another transaction has written but not yet committed**.  If that other transaction rolls back (undoes) the change, the first transaction has read **invalid or “dirty” data**.

|Transaction A|Transaction B|
|---|---|
|BEGIN TRANSACTION;||
|UPDATE accounts SET balance = 200 WHERE id = 1;||
|_(not yet committed)_||
||BEGIN TRANSACTION;|
||SELECT balance FROM accounts WHERE id = 1; → **200**|
||-- B reads the uncommitted value|
|ROLLBACK;||
|-- balance goes back to 100||
||COMMIT;|
||-- B has used wrong data (200) that never really existed


2. Non repetable reads: A non-repeatable read occurs when **a transaction reads the same record twice and gets different values**, because another transaction **modified or deleted** that record in between the reads.

| Transaction A                                        | Transaction B                                   |
| ---------------------------------------------------- | ----------------------------------------------- |
|                                                      |                                                 |
| BEGIN TRANSACTION;                                   |                                                 |
| SELECT balance FROM accounts WHERE id = 1; → **100** |                                                 |
|                                                      | BEGIN TRANSACTION;                              |
|                                                      | UPDATE accounts SET balance = 200 WHERE id = 1; |
|                                                      | COMMIT;                                         |
| SELECT balance FROM accounts WHERE id = 1; → **200** |                                                 |
| COMMIT;                                              |                                                 |


3. Phantom reads: A phantom read occurs when **new rows appear (or existing rows disappear)** between two reads in the same transaction not because a row changed, but because **another transaction inserted or deleted rows** that match the query condition.

| Transaction A                                                          | Transaction B                         |
| ---------------------------------------------------------------------- | ------------------------------------- |
|                                                                        |                                       |
| BEGIN TRANSACTION;                                                     |                                       |
| SELECT * FROM accounts WHERE balance > 100; → returns **(id=2)**       |                                       |
|                                                                        | BEGIN TRANSACTION;                    |
|                                                                        | INSERT INTO accounts VALUES (3, 200); |
|                                                                        | COMMIT;                               |
| SELECT * FROM accounts WHERE balance > 100; → returns **(id=2, id=3)** |                                       |
| COMMIT;                                                                |                                       |

READ uncommited does not solve any of the issues.

#### Read commited

A transaction can **only read committed data** uncommitted updates from others are invisible. 

So it solves the problem of dirty reads. However while this transaction is running any other transation can update the row so It does not solves no repetable reads. Similary since other transaction can still insert or delete a row while this is going on so phantom read is still a problem. 

#### Repeatable read

Once you read a row, **no other transaction can modify that row** until you finish (depending on DB engine).You’ll always get the same values if you re-read the same rows.However, **new rows** that match your query condition can appear later  this is the **phantom read** problem.

#### Serializable

Transactions are executed **as if they were completely sequential** — one after another.  
The database ensures no other transaction affects your results, even indirectly.

Prevents: Dirty reads, non-repeatable reads, phantom reads. It is like doing each transaction one by one and this is achieved by **locks** or **MVCC snapshots**.

#### Durability

Once PostgreSQL says a transaction is committed, that data is guaranteed to have been written to disk and sync'd, protecting against crashes or power failures. This is achieved through Write-Ahead Logging (WAL):

1. Changes are first written to a log
2. The log is flushed to disk
3. Only then is the transaction considered committed

### Concurrency control mechanisms 

#### Pessimistic locking

Pessimistic locking means locking the resource before hand before using it. LOcking requries multiple entries to be maanged by the database and thus is slow. In case of no contention it should not be used. 

**Shared Locks (S-Locks):** Used for reading. Multiple transactions can hold a shared lock on the same resource.

**Exclusive Locks (X-Locks):** Used for writing. Only one transaction can hold this lock, and no one else can even read the data until the lock is released.

**Two-Phase Locking (2PL):** A protocol ensuring that a transaction acquires all its locks in a "growing phase" and releases them in a "shrinking phase," preventing consistency issues.


During phase 1, the transaction is allowed to acquire as many locks as it needs.
- It can request **Shared Locks (S)** for reading.
- It can request **Exclusive Locks (X)** for writing.
- It can "upgrade" a lock (turn a Read lock into a Write lock).
- **Constraint:** It cannot release _any_ locks during this phase.


This phase 2 (shrink) begins the moment the transaction releases its first lock.
- The transaction can release locks it no longer needs.
- It can "downgrade" a lock (turn a Write lock into a Read lock).
- **Constraint:** It is strictly forbidden from acquiring any new locks or upgrading existing ones.

This is done to ensure serializability. However this can introduce dead locks. `Req1 picks AB and Req2 picks BA`. Solution is do a global ordering of resources so only both req will then have to take the requests in same order. This breaks circular wait.  

**Best for:** Environments with **high contention** (many users fighting for the same rows) where the cost of rolling back a transaction is much higher than the cost of waiting for a lock.

#### Optimisatic locking 

This technique is helpfull when the conflicts are rare. The user assumes that since conflicts are rare it first tries to do the update but before commiting it checks if the resource has changed if yes then it reverts the transaction. 

**The Three Phases:**

1. **Read Phase:** The transaction reads the data and its current version number (e.g., Version 5). It performs all calculations in a local "private workspace."
2. **Validation Phase:** When the user hits "Save," the database checks: _"Is the version in the database still Version 5?"_
3. **Write Phase:** * If **Yes**: The data is updated and the version is bumped to 6.
    - If **No**: Someone else changed it while you were working. Your transaction is **aborted**, and you have to start over.

**Best for:** **Low-contention** environments (like a CMS where two people rarely edit the same article at the exact same time). It is highly "performant" because there is zero locking overhead.

#### Multiversion concurrency control

It is technically a form of optimistic control, but it is so robust that it is treated as its own category. Its mantra is: **"Readers should never block writers, and writers should never block readers."**

**How it works:** Instead of overwriting a row, the database creates a **new version** of that row. 
- Every row has a "Valid From" and "Valid To" timestamp (or Transaction ID).
- When you start a transaction, the DB gives you a **Snapshot**. You only see versions of data that were committed _before_ your start time.

**The Workflow:**
1. User A starts reading a row (Version 1).
2. **User B** starts an update on that same row. Instead of locking User A out, User B creates **Version 2**.
3. **User A** continues reading Version 1. They are completely unaware that Version 2 is being built.
4. **User B** commits. New readers will now see Version 2.
5. **Garbage Collection:** Later, a background process (like Postgres's 

`VACUUM`) deletes Version 1 once no active transactions need it anymore.
- **Best for:** Almost everything. This is why it’s the default in **PostgreSQL, Oracle, and SQL Server** (in certain modes). It provides high concurrency by allowing reads and writes to happen simultaneously.
- **The Downside:** **Storage bloat**. Since you are keeping multiple copies of the same data, the database uses more disk space and requires background maintenance to clean up old versions.

