# S3

Amazon **S3 (Simple Storage Service)** is AWS’s highly durable, scalable, and secure object storage service designed to store and retrieve any amount of data from anywhere on the internet. It is one of the foundational services in AWS and is widely used by organizations of all sizes because of its simplicity, reliability, and cost-effectiveness. S3 stores data as **objects** inside **buckets**, making it ideal for use cases such as backups, media storage, analytics, machine learning datasets, and application data.

One of the defining features of S3 is its **unlimited scalability**. You do not need to provision storage space—S3 automatically grows as you upload more data. This “infinite storage” capability makes it suitable for large-scale data lakes, enterprise backups, and content archives. Along with this, S3 offers **11 nines (99.999999999%) durability**, achieved through automatic replication of data across multiple devices and Availability Zones. This ensures that the data is protected against hardware failures and disasters, without requiring any manual configuration.

S3 allows people to store objects (files) in buckets(directories). Buckets must have globally unique name across globe. Its a region level service though. Bucket is created at region level.

Objects are the files. Each object has a key which is a full path of a file - Key is composed of prefix and an object name

```
s3://my_bucket/my_folder/my_file.txt

```

There is no concept of directories in s3. Keys although trick you to think so. Max object size is 5TB However if uploading more than 5gb we need to use multipart download.

Beyond storage, S3 supports powerful features such as **versioning**, which keeps multiple versions of an object to protect against accidental deletions or overwrites. **Lifecycle policies** allow automated transitions between storage classes or automated deletion of old data. **Event notifications** can trigger Lambda functions or send messages to SNS/SQS when objects are uploaded or modified, enabling serverless workflows and real-time processing. S3 also supports hosting of **static websites**, making it ideal for serving lightweight web applications.

There are two kinds of buckets -

- General purpose
- Directory new - Faster for low latency

A **pre-signed URL** is a time-limited URL that gives **temporary, secure access** to an S3 object **without requiring AWS credentials**.

When you generate it, you essentially sign the request with your AWS credentials and specify:

- **Which S3 object** can be accessed
- **What action** is allowed (GET / PUT)
- **How long** the access is valid

## Security

User based - IAM policies - which api calls should be allowed for specific user from IAM.
Resource based -
1. bucket wide rules from s3 console
2. Object ACL (access control list) - finer grained

Note that most common way to configure access on bucket is using bucket policy.
Now an IAM principal can access S3 object if

- User IAM permission allows it or resource policy ALLOWS it and there is no explicit deny.
- S3 bucket policies are JSON structures

```json
{
    "version":"2017-10-17",
    "statrement":[
    {
        "Sid":"Public Read",
        "Effect":"Allow",
        "Principal":"*",
        "Action":[
        "s3:getObject"
        ],
        "Resource":[
        "arn:aws..name/*"
        ]
    }
    ]
}

```

EC2 instances can access the s3 bucket using IAM roles.

S3 can be used to host the static websites but make sure to have public read allowed to all the objects.

**S3 Versioning** is a feature that lets Amazon S3 **keep multiple versions of the same object** in a bucket.
This means:

- Every time you upload a new file **with the same key/name**, S3 **creates a new version**.
- You can **restore**, **retrieve**, or **delete** specific versions anytime.

When you delete an object in a versioned bucket:

- S3 **does NOT remove the object**
- Instead, it adds a **delete marker version**
- The object seems deleted in the console, but previous versions are intact

To restore → just delete the delete marker.

## Website hosting

Normally when you go to website. Links redirect to different actual files in S3. If user goes to `/about.html`, S3 directly serves **that exact file**.
S3 knows exactly which file to serve because **every route corresponds to an actual file**.

However with react there is only one html document and routes are rendered client side.
React apps use **client-side routing** (React Router), meaning:

- The browser handles the route
- The server (S3) should **always return index.html**, no matter the URL

So if S3 cannot find a file for a route, it must still return **index.html**, so React can load and show the correct page. So we need to configure the error page as well to show the index.html

## Replication

Amazon S3 Replication is a managed feature that automatically copies objects from one S3 bucket to another. It works **asynchronously**, meaning objects are replicated shortly after they are uploaded. S3 Replication is commonly used to meet compliance requirements, improve data availability, enable low-latency access across regions, or maintain separate production and backup environments. Replication requires enabling **versioning** on both the source and destination buckets. Buckets can be of different accounts.

There are **two main types of S3 replication**:

**1. Cross-Region Replication (CRR)**

CRR replicates objects **across AWS Regions**, such as from `us-east-1` to `ap-south-1`. This provides geographical redundancy and helps reduce latency for global users. It’s widely used for disaster recovery, compliance, and global content distribution.

**2. Same-Region Replication (SRR)**

SRR replicates objects **within the same region**. It’s ideal for maintaining separate buckets for logging, security isolation, or keeping production and analytics data apart. SRR is also used for data sovereignty, where data must remain in one region but exist in multiple managed copies.

## **S3 storage classes**

| Storage Class                  | Access Frequency | Redundancy | Cost         | Retrieval Time            |
| ------------------------------ | ---------------- | ---------- | ------------ | ------------------------- |
| **Standard**                   | Frequent         | Multi-AZ   | High         | Instant                   |
| **Standard-IA**                | Infrequent       | Multi-AZ   | Medium       | Instant (+ retrieval fee) |
| **One Zone-IA**                | Infrequent       | Single AZ  | Low          | Instant                   |
| **Intelligent-Tiering**        | Unknown          | Multi-AZ   | Auto-managed | Instant                   |
| **Glacier Instant Retrieval**  | Rare             | Multi-AZ   | Low          | Instant                   |
| **Glacier Flexible Retrieval** | Very rare        | Multi-AZ   | Very low     | Minutes–hours             |
| **Glacier Deep Archive**       | Almost never     | Multi-AZ   | Lowest       | 12–48 hours               |
S3 Standard is the default storage class used for frequently accessed data. It provides **high availability (99.99%)**, **low-latency retrieval**, and supports a broad range of workloads such as websites, streaming, mobile apps, data pipelines, and machine learning workloads. Because it is optimized for rapid access, it is the most expensive tier, but it provides the best performance.

S3 Intelligent-Tiering automatically moves objects between access tiers based on usage patterns. It does **not charge retrieval fees**, and offers the same high durability and low-latency access as S3 Standard. It monitors your data and moves it to cheaper storage if it detects that data is not being accessed frequently.

It has four tiers:

- Frequent Access
- Infrequent Access
- Archive Instant Access
- Archive (and Deep Archive tiers if enabled manually)

S3 Standard-IA is designed for data that needs **low-latency access** but is not accessed frequently. It has lower storage cost but charges **retrieval fees**, so it's best suited for backups, disaster recovery copies, and infrequently accessed business data.

**Use when:**
• Long-lived data that is rarely accessed
• Disaster recovery backups
• Data you may need immediately when required

Unlike most S3 storage classes, **One Zone-IA stores data in a single Availability Zone** instead of multiple AZs. This makes it cheaper, but with lower availability and no multi-AZ redundancy.

It is still very durable but should not be used for mission-critical data.

S3 glacier instant retrival storage class offers very low storage cost with **millisecond retrieval**, similar to S3 Standard/IA retrieval times. It’s ideal for data that is accessed **once or twice a quarter**, but still requires instant access when needed. It has a **minimum storage duration of 90 days**.

S3 Glacier Flexible Retrieval is designed for long-term archives with occasional access. It offers **minutes or hours retrieval time**, depending on the retrieval tier (expedited, standard, or bulk). It is cheaper than Instant Retrieval and has a **90-day minimum storage duration**.

Deep Archive is the **cheapest storage class in S3**, designed for long-term cold storage. Retrieval times are slower (10–12 hours standard, or ~1 hour for bulk/expedited), and it has a **180-day minimum storage requirement**.

Durability in all of them is 11 9s.

## **Amazon S3 Express One Zone**

Amazon **S3 Express One Zone** is the **newest, fastest, and highest-performance** storage class in S3, designed specifically for workloads that need **ultra-low latency** and **very high throughput**. Unlike traditional S3 storage classes, S3 Express One Zone stores data in **a single AWS Availability Zone** and delivers performance similar to local file systems or block storage—but with the scalability and durability of S3.

It is ideal for high-performance applications such as ML training, data analytics engines, and real-time data processing.

Traditional S3 provides very high durability and availability but isn’t designed for **ultra-fast metadata operations** (creating, listing, updating millions of objects per second). Modern workloads like machine learning pipelines or analytics frameworks require extremely **high request rates** and **sub-millisecond access**.

S3 Express One Zone solves this by providing a **fast-access directory bucket** that stores data close to compute resources in the same AZ.

## **S3 encryption**

Server side encrytion is by default where once an object is uploaded to S3 server will encrypt the files before receiving it.
In client side encryption the user will encrypt the object before upload.

IAM Access analyser for S3 is used to find about which ppl have what access to your S3 bucket.

### Shared responsiblity model

AWS is responsible for

1. Infra  - (global security , durability, availability )
2. Config and vulnerability analysis
3. Compliance validation

User is responsible for

1. S3 versioning
2. S3 bucket policies
3. S3 replication setup

## AWS snow family

The **AWS Snow Family** is a collection of physical, rugged devices used to **move very large amounts of data** into or out of AWS, especially when transferring over the internet is too slow, expensive, or not feasible. These devices are physically shipped to customers, allowing data to be copied locally and then moved securely to AWS.

They also support **edge computing**, meaning you can run compute workloads in disconnected or harsh environments such as ships, factories, mines, or military locations.

Transferring petabytes of data to AWS over the internet can take **weeks or months**, even with high-speed connections. In remote locations, internet connectivity may not exist at all. The Snow devices solve this by providing **offline, secure, high-capacity data transfer** using physical appliances.

**Smallest and lightest Snow device**

- **8 TB (HDD version)**
- **14 TB (SSD version)**
- Portable (weighs ~2 kg)
- Can run edge compute workloads using **AWS IoT Greengrass** or **EC2 instances**.

AWS snowball edge is the mid size device of family and has two main variations.

1. Snowball edge storage optimised.
2. Snoball edge compute optimised.

Finally we have snowmobile which is a container and is pulled by truck.

## Storage gateway

AWS Storage Gateway is a **hybrid cloud storage service** that connects your on-premises environment (datacenters, offices, factories) to AWS cloud storage like **S3, Glacier, EBS, FSx**, etc. It allows organizations to keep using their **existing on-prem applications and storage workflows**, while silently extending the backend storage to AWS.

There are main tree types of storage gateway -

- File gateway(NFS to S3)
- Volume gateway(Cached and sotred volume)
- Tape gateway (virtual tape library)
