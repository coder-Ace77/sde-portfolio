---
title: "5. Networking"
description: ""
date: "2026-02-05"
---



Each pods gets its own unique clusterwide ip address. A pod has its own private network namespace which is shared by all of the containers within the pod. Processes running in different containers in the same pod can communicate with each other over `localhost`

The _pod network_ (also called a cluster network) handles communication between pods. It ensures that (barring intentional network segmentation): 

All pods can communicate with all other pods, whether they are on the same node or on different nodes. Pods can communicate with each other directly, without the use of proxies or address translation (NAT).  If Pod A and Pod B are on the same machine, traffic goes through a virtual bridge. If Pod A is on Node 1 and Pod C is on Node 2, Kubernetes relies on a **CNI (Container Network Interface)** plugin like Calico, Flannel, or Cilium to "tunnel" the traffic across the physical network.

Now pods start and stop and each time ips change. To get one stable api we use at simplest service ip. A Service gets a stable "Virtual IP" (ClusterIP). When a Pod sends traffic to that Virtual IP, `kube-proxy` (running on every node) intercepts the traffic and load-balances it across the actual Pods matching the Service's labels. This is called `East-west` networking. 

Now finally we need to find out how to connect External to service connection which is called north-south networking. There are three ways to do this

| **Type**         | **How it Works**                                                            | **Best Use Case**                                                  |
| ---------------- | --------------------------------------------------------------------------- | ------------------------------------------------------------------ |
| **NodePort**     | Opens a specific port (30000-32767) on every Node's physical IP.            | Simple testing/internal tools.                                     |
| **LoadBalancer** | Provisions a "real" Load Balancer from your cloud provider (AWS/GCP/Azure). | Standard way to expose a service to the internet.                  |
| **Ingress**      | A "smart" router that sits in front of multiple services.                   | Routing based on URLs (e.g., `api.com/users` vs `api.com/orders`). |

Finally kubernetes has build in dns service so that we can resolve the urls like `http://my-app` to actual port and ip. You don't even need to know the Service's ClusterIP. Kubernetes has a built-in DNS service (CoreDNS). If you create a Service named `my-database`, any Pod in the same namespace can simply reach it by typing `http://my-database`. Kubernetes DNS automatically resolves that name to the Service's internal IP.

### Service 

Expose an application running in your cluster behind a single outward-facing endpoint, even when the workload is split across multiple backends. Note that internally if you are using deployments to have two set of applications like `frontend` and `backend`. We should not use directly the ips of pods reason being pods can get destroyed and restarted. Instead inside the cluster we must use service enddpoint to communicate.

This leads to a problem: if some set of Pods (call them "backends") provides functionality to other Pods (call them "frontends") inside your cluster, how do the frontends find out and keep track of which IP address to connect to, so that the frontend can use the backend part of the workload?

The Service API, part of Kubernetes, is an abstraction to help you expose groups of Pods over a network. Each Service object defines a logical set of endpoints (usually these endpoints are Pods) along with a policy about how to make those pods accessible.

The set of Pods targeted by a Service is usually determined by a [selector](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/) that you define.
If your workload speaks HTTP, you might choose to use an [Ingress](https://kubernetes.io/docs/concepts/services-networking/ingress/) to control how web traffic reaches that workload. Ingress is not a Service type, but it acts as the entry point for your cluster. An Ingress lets you consolidate your routing rules into a single resource, so that you can expose multiple components of your workload, running separately in your cluster, behind a single listener.

A Service is an [object](https://kubernetes.io/docs/concepts/overview/working-with-objects/#kubernetes-objects) (the same way that a Pod or a ConfigMap is an object). You can create, view or modify Service definitions using the Kubernetes API. Usually you use a tool such as `kubectl` to make those API calls for you.

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app.kubernetes.io/name: MyApp
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
```

Applying this manifest creates a new Service named "my-service" with the default ClusterIP [service type](https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types). The Service targets TCP port 9376 on any Pod with the `app.kubernetes.io/name: MyApp` label.

For some parts of your application (for example, frontends) you may want to expose a Service onto an external IP address, one that's accessible from outside of your cluster.

Kubernetes Service types allow you to specify what kind of Service you want. The available `type` values and their behaviors are: 

ClusterIP:

Exposes the Service on a cluster-internal IP. Choosing this value makes the Service only reachable from within the cluster. This is the default that is used if you don't explicitly specify a `type` for a Service. However we can still expose this using ingress.

NodePort:

Exposes the Service on each Node's IP at a static port (the `NodePort`). To make the node port available, Kubernetes sets up a cluster IP address, the same as if you had requested a Service of `type: ClusterIP`. SO the service will be available at the `<NodeIP>:<NodePort>`. 

```yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  selector:
    app: my-nginx    # This MUST match the label in your Deployment template
  ports:
    - protocol: TCP
      port: 80       # The port the Service listens on
      targetPort: 80 # The port the Pods are listening on
  type: NodePort     # This makes it accessible from outside the cluster
```

Quick demonstrations, or cases where you have a legacy external load balancer that isn't integrated with Kubernetes.

Loadbalancer

Exposes the Service externally using an external load balancer. 
The cloud provider spins up an external Load Balancer (like an AWS ELB) that gets its own public IP address. It automatically creates a `NodePort` and a `ClusterIP` behind the scenes, then tells the cloud's external load balancer to route traffic to those NodePorts.
Kubernetes does not directly offer a load balancing component; you must provide one, or you can integrate your Kubernetes cluster with a cloud provider.

```yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx-external-service
spec:
  type: LoadBalancer    # This triggers the cloud provider's LB creation
  selector:
    app: my-web-app     # This MUST match the label in your Pod metadata
  ports:
    - protocol: TCP
      port: 80          # The port the Load Balancer listens on (External)
      targetPort: 80    # The port your Nginx container is listening on (Internal)
```


### Ingress

Make your HTTP (or HTTPS) network service available using a protocol-aware configuration mechanism, that understands web concepts like URIs, hostnames, paths, and more. The Ingress concept lets you map traffic to different backends based on rules you define via the Kubernetes API. 

Ingress exposes http/https routes from outside the cluster to services within cluster. 
An Ingress may be configured to give Services externally-reachable URLs, load balance traffic, terminate SSL / TLS, and offer name-based virtual hosting. An [Ingress controller](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/) is responsible for fulfilling the Ingress, usually with a load balancer, though it may also configure your edge router or additional frontends to help handle the traffic.

One must have ingress controller to satisfy ingress. Having ingress idrectly is of no help. 

Minimal ingress example

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: minimal-ingress
spec:
  ingressClassName: nginx-example
  rules:
  - http:
      paths:
      - path: /testpath
        pathType: Prefix
        backend:
          service:
            name: test
            port:
              number: 80
```

An Ingress needs `apiVersion`, `kind`, `metadata` and `spec` fields. The name of an Ingress object must be a valid [DNS subdomain name](https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names). The [Ingress spec](https://kubernetes.io/docs/reference/kubernetes-api/service-resources/ingress-v1/#IngressSpec) has all the information needed to configure a load balancer or proxy server. Most importantly, it contains a list of rules matched against all incoming requests. Ingress resource only supports rules for directing HTTP(S) traffic.

Each HTTP rule contains the following information:

An optional host. In this example, no host is specified, so the rule applies to all inbound HTTP traffic through the IP address specified. If a host is provided (for example, foo.bar.com), the rules apply to that host.

A list of paths (for example, `/testpath`), each of which has an associated backend defined with a `service.name` and a `service.port.name` or `service.port.number`. Both the host and path must match the content of an incoming request before the load balancer directs traffic to the referenced Service.

Path can be of two types - 

`Exact` - Match the entire url
`Prefix` - Matches based on a URL path prefix split by `/`.
