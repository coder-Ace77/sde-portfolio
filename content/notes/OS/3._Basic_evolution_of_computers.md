---
title: "3. Basic evolution of computers"
description: ""
date: "2026-02-05"
---



### Multiprogramming

Inital computers were simple they would start execution with one program and make it complete and then run second one. During its execution if CPU was required to wait for I/O CPU cycles were wasted. From here came *multiprogramming* which means instead of keeping only one program in memory multiple programs are kept in the memory. . When one program had to wait  for disk I/O, for example the CPU would immediately switch to another program that was ready to run. The programs were not running at the same instant, but they were taking turns so fast that the CPU was almost never idle. 

### Time sharing

As computers spread to universities and offices, another problem appeared. Multiple people wanted to use the same machine at the same time. Batch jobs were no longer enough. People wanted to sit at terminals, type commands, and get responses interactively. This led to **time-sharing**.

Time-sharing took multiprogramming and added a human-friendly twist. The operating system would switch between programs at fixed time intervals, even if they didn’t block on I/O. Each program would get a small slice of CPU time, then be paused so another could run. The switches were so fast that each user felt like they had the whole machine to themselves. This is where the idea of “my program is running while yours is also running” really entered everyday thinking, even though under the hood it was still mostly one CPU executing one instruction at a time.

Now, for a long time, this switching was still happening on **a single CPU**. True parallel execution wasn’t happening yet. That changed when hardware evolved.

#### Multiprocessing

As processors became more powerful, engineers realized that increasing clock speed alone wasn’t enough. Heat, power consumption, and physical limits got in the way. The solution was to put **multiple CPUs**, or later **multiple cores**, into the same machine. This is where **multiprocessing** enters the story.

Multiprocessing means the system has **more than one processor**, and therefore more than one instruction stream can execute at the exact same time. Unlike multiprogramming or time-sharing, this is not an illusion. Two programs can literally run in parallel. Or two parts of the same program can run in parallel. This is real, physical concurrency.

### Process vs thread

At this point, the operating system had a much harder job. It now had to decide not only _which_ program to run, but also _where_ to run it — on which CPU core. Scheduling, synchronization, and shared memory suddenly became serious problems. Bugs like race conditions and deadlocks emerged because multiple things were truly happening at the same time.

This is also where the distinction between **processes** and **threads** becomes important.

A process is like a self-contained world. It has its own memory, its own resources, and its own identity. In early multiprogramming systems, each program was essentially one process. Context switching between processes was expensive because memory had to be protected and isolated.

Threads were introduced to make concurrency cheaper. A thread is a smaller execution unit inside a process. Threads share memory and resources but have their own execution paths. This made it possible for a single program to do multiple things at once — for example, one thread handling user input while another performs background computation.


### Concurrency

As systems became more complex, another idea surfaced: **concurrency is not the same as parallelism**.

Concurrency is about **structure**. It’s about designing a system where multiple tasks are in progress at the same time conceptually. Parallelism is about **execution**. It’s about multiple tasks actually running at the same time physically. You can have concurrency without parallelism (one CPU switching tasks), and parallelism without much concurrency (a single task split across many cores).

### Multitasking

As computers moved from research labs into offices and homes, something important changed. The machine was no longer serving only engineers who submitted jobs and waited for results.It was now serving **people sitting in front of a screen**, clicking, typing, listening to music, and expecting instant response. From the human point of view, the computer had to _do many things at once_. That expectation is what we call **multitasking**.

Multitasking is not a low-level technical trick like multiprocessing or multithreading. It is a **user-level illusion** and an operating system promise. The promise is simple: you can run multiple tasks at the same time, and none of them should feel blocked by the others.

Under the hood, the operating system achieves multitasking using the ideas that came before it. On a single-core machine, multitasking is built on **time-sharing**. The CPU rapidly switches between tasks, giving each one a small slice of time. The switching is so fast that your brain perceives simultaneity. The browser renders a frame, the music player fills its audio buffer, the download thread waits on the network, and the editor reacts to keystrokes — all by taking turns.

On a multi-core machine, multitasking becomes even more convincing. Some tasks truly run at the same time on different cores, while others are still time-shared. The illusion becomes partially real.

Modern systems use **preemptive multitasking**. The operating system is in charge. It can interrupt a running task at any time, save its state, and switch to another one.

### Dual mode

In order to provide the security and proper execution we must be able to identify the OS code and user code. The operating system runs in one mode with full authority, and user programs run in another mode with limited authority. The CPU itself enforces this separation; it is not just a software convention.

When the computer starts, the operating system boots and the CPU is placed in **kernel mode** (also called supervisor or privileged mode). In this mode, the OS has complete control. It can talk directly to hardware, manage memory, schedule processes, configure devices, and execute special instructions that ordinary programs are forbidden to use. Think of kernel mode as the “control room” of the system—only trusted staff are allowed inside.

Once the OS is initialized, it starts user applications. These applications run in **user mode**. In user mode, programs can execute normal instructions like arithmetic operations, loops, and function calls, but they are _restricted_. They cannot directly access hardware, they cannot modify critical memory regions, and they cannot execute privileged CPU instructions. This restriction is what keeps the system stable and secure.

Now, here’s the interesting part: user programs _still need_ services that only the OS can provide. For example, a program might want to read a file, send data over the network, or allocate more memory. All of these actions require privileged access.

When a user program needs an OS service, it doesn’t directly access hardware. Instead, it makes a system call. A system call is like a formal request: “Dear OS, please do this for me.” Technically, the program executes a special instruction that triggers a controlled switch from user mode to kernel mode. The CPU checks that this transition is valid, switches modes, and hands control to a predefined OS routine.

