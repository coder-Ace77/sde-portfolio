---
title: "9.Memory management"
description: ""
date: "2026-02-05"
---



### Program to process

The process starts with the **source program**, which is written using symbolic names such as variable names and function names. During **compile time**, the compiler or assembler translates this high-level or assembly code into an **object module**. At this stage, addresses are not final memory addresses; they are still symbolic or relative, because the exact memory location of the program is not yet known.

Next, during **link time**, the **linker** combines the object module generated from the source program with **other object modules** and external references. These may include user-defined modules or precompiled library code. The output of the linker is a **load module**, where addresses are converted into **relocatable addresses**. Relocatable addresses mean that the program can be loaded at different memory locations, and its internal references can still work correctly once adjusted. This step allows modular programming and reuse of code across different programs.

Next, during **link time**, the **linker** combines the object module generated from the source program with **other object modules** and external references. These may include user-defined modules or precompiled library code. The output of the linker is a **load module**, where addresses are converted into **relocatable addresses**. Relocatable addresses mean that the program can be loaded at different memory locations, and its internal references can still work correctly once adjusted. This step allows modular programming and reuse of code across different programs.

CPU only knows about the logial or virtual address. It is the task of MMU(Memory management unit) to translate the logical address to physical address. The MMU uses a **base register**, which stores the starting physical address of the process in memory, and adds it to the logical address to produce a **physical address**. This physical address is then used to access main memory. This mechanism provides **memory protection and relocation**, ensuring that a process accesses only its own allocated memory region.

The data or instruction must first be residing into main memory for it to get executed. Registers of cpu are accessible in one cpu cycle.Completing a memory access may take many cycles of the CPU clock thus processor may need to wait while data is available. For proper system operation we must protect the operating system from access by user processes. We also need to make sure that each process has a separate memory space. Separate per-process memory space protects the processes from each other and is fundamental to having multiple processes loaded in memory for concurrent execution. 

### Swapping 

A process must be in memory to be executed. A process, however, can be swapped temporarily out of memory to a backing store and then brought back into memory for continued execution. Swapping makes it possible for the total physical address space of all processes to exceed the real physical memory of the system, thus increasing the degree of multiprogramming in a
system. Swapping is reqrired when a process wants to execute but not enough memory is available thus a current process is taken out of memory into backing store. 

The total time of swap is dependent mostly on the amount of memory to be swapped beause of slow speed of disk. Standard swapping however requires too much swapping time and gives us too little execution time. Other modified variations exist which stop swapping until say available memory is below threahold. 

### Memory allocation

The main memory must accommodate both the operating system and the various user processes.The memory is usually divided into two partitions: one for the resident operating system and one for the user processes. We can place the operating
system in either low memory or high memory. The major factor affecting this decision is the location of the interrupt vector. Since the interrupt vector is often in low memory, programmers usually place the operating system in low memory as well.

Now there are various schemes when allocating memory. 

One of the simplest methods for allocating memory is to divide memory into several fixed-sized partitions. Each partition may contain exactly one process. Thus, the degree of multiprogramming is bound by the number of partitions. In this multiple- partition method, when a partition is free, a process is selected from the input queue and is loaded into the free partition.

In the variable-partition scheme, the operating system keeps a table indicating which parts of memory are available and which are occupied. Initially, all memory is available for user processes and is considered one large block of available memory, a hole. Eventually, as you will see, memory contains a set of holes of various sizes.

Now next process will ocuppy which hole is to be determined with care-

- First-fit - Allocate the first hole that is big enough. Searching can start either at the beginning of the set of holes or at the location where the previous first-fit search ended. We can stop searching as soon as we find a free hole that is large enough.
- Best fit. Allocate the smallest hole that is big enough. We must search the entire list, unless the list is ordered by size. This strategy produces the smallest leftover hole.
- Worst fit. Allocate the largest hole. Again, we must search the entire list, unless it is sorted by size. This strategy produces the largest leftover hole, which may be more useful than the smaller leftover hole from a best-fit approach.
#### Memory protection

Memory of a user space should not be accessible by another process. This is ensured using two addresses `limit` and `base`. `Base` register consists of the smalllest addressable unit and `limit` is used to find the range. Each logical address must fall within the range specified by the limit register. When the CPU scheduler selects a process for execution, the dispatcher
loads the relocation and limit registers with the correct values as part of the context switch. Because every address generated by a CPU is checked against these registers, we can protect both the operating system and the other users’
programs and data from being modified by this running process.

![Pasted image 20260111161116.png](/notes-images/Pasted%20image%2020260111161116.png)

### Fragmentation

Fragmentation means division of large memory into smaller holes. 

External fragmentation means there is enough total memory to satisfy the request but it is not continous. In the worst case, we could have a block of free (or wasted) memory between every two processes. Both first fit and best-fit suffer from external fragmentation. Statistical analysis of best and first fit shows that for every block of memory allocated half block is unusable. 

With the fixed size block scheme we can alllocate some additional memory to a process which is not usaed. This unused memory is called internal fragmentation aka fragmentation internal to the partitition.

### Segmenation

Segmentation is a memory-management scheme that supports this programmer view of memory. A logical address space is a collection of segments.

A programer does not sees memory as the continous units of bytes but as the collection of segments. Each segment has a name and a length. The addresses specify both the segment name and the offset within the segment. The programmer therefore specifies each address by two quantities: a segment name and an offset.

In this view of momory management technique logical address is defined by `<segment-number,offset>`

![Pasted image 20260111162648.png](/notes-images/Pasted%20image%2020260111162648.png)

Now here logcial address is 2d but ofcourse real memory is 1d. So a  mapping called segmentation table is provided which acts as the mapping from 2d address space to 1d address space. Each entry in the segment table has a segment base and a segment limit. The segment base contains the starting physical address where the segment resides in memory, and the segment limit specifies the length of the segment.

![Pasted image 20260111162939.png](/notes-images/Pasted%20image%2020260111162939.png)

With segmentation we can divide the actual process into multiple non continous blocks. 

### Paging

Paging is the solution to external fragmentation. Physical memory address space is divided into fixed size blocks called frames and breaks logical address space into fixed size called pages.Frames and pages are of equal size.  When a process is to be executed, its pages are loaded into any available memory frames from their source (a file system or the backing store). The backing store is divided into fixed-sized blocks that are the same size as the memory frames or clusters of multiple frames.

Paging totally separates the logical address space from physical one so not we can have 64 bit address space even if we have lesser memory. 

The logical address generated by cpu is divided into two parts-

- page number(p)
- page offset(d)

If page size is `2^n` then offset will have n bits and all other higher bits of address space will belong to page number.
A **page table** is a data structure maintained by the OS for each process. It maps **page numbers → frame numbers**.

Each page table entry (PTE) typically contains:
- **Frame number**
- **Valid/Invalid bit** (whether page is in memory)
- **Protection bits** (read/write/execute)
- **Dirty bit** (whether page was modified)
- **Reference bit** (used in page replacement)

![Pasted image 20260111165104.png](/notes-images/Pasted%20image%2020260111165104.png)

 Address Translation Using Paging
1. CPU generates a logical address ⟨page number, offset⟩
2. Page number is used to index the page table
3. Frame number is fetched from the page table
4. Physical address = ⟨frame number, offset⟩
5. Memory is accessed using the physical address

This translation is done by the **Memory Management Unit (MMU)**.
The **MMU** performs:
- Logical → physical address translation
- Memory protection checks
- Detection of invalid accesses (page faults)

A **page fault** occurs when a process accesses a page whose valid bit is 0 (i.e., the page is not in main memory).

Steps during a page fault:
1. CPU traps to the OS
2. OS checks if the access is valid
3. Required page is fetched from disk (swap space)
4. A free frame is allocated (or a victim page is replaced)
5. Page table is updated
6. Instruction is restarted

In **demand paging**, pages are loaded into memory **only when they are needed**, not at process start. This:

- Reduces memory usage
- Improves multiprogramming
- Speeds up program startup

With paging we have no external fragmentation but internal fragmentation may still exist. Atmost one page of wastage per process is possible with pagination. This means smaller pages are desirable but with smaller pages frequency of swapping increases. So we need to maintain balance. 

OS also keeps another data structure called `frame table`. frame table has one entry for each physical page frame indicating weather latter is free or allocated if it is allocated, to which page of which process or processes.

When memory is full and a page fault occurs, the OS must choose a page to replace. A page table base register (PTBR) points to the page table. Changing the page table requires just changing the value of the register, thus reducing the context
switch time.

Page table is usally kept in main memory due to its large size and there exist one page table base egister(PTBR) pointing to page table. Changing page table requires one registe time reducing context switch time. With this scheme however two memory access(one for page table) are needed per byte access. 

The solution is to use **TLB(translation lookaside buffer)**.The TLB is associative, high-speed memory. 

Each entry in the TLB consists of two parts: 

A key (or tag) and a value. When the associative memory is presented with an item, the item is compared with all keys simultaneously. If the item is found, the corresponding value field is returned. The search is fast; a TLB lookup in modern hardware is part of the instruction pipeline, essentially adding no performance penalty.

If pagentry is present in tlb then its called TLB hit and otherwise TLB miss. The percentage of time hit has happened is called hit ratio. 

Protection in paged environment is accomplished by protection bits in each frame. One bit can define the page to read-only,write-only etc. One additional bit is generally attached to each entry in page table called valid-invalid bit. When this bit is set to valid, the associated page is in the process’s logical address space and is thus a legal (or valid) page. When the
bit is set toinvalid, the page is not in the process’s logical address space. Illegal addresses are trapped by use of the valid –invalid bit.

Paging concept is also helpful in keeping the shared pages. Since we can keep the shared pages in the memory and every process can map to the same frame. This will help us in using lesser memory. For example in case of text editor. for each instance of editor we can have same code and text section and thus can be shared. 

### Structure of page table

Modern computer systems can have logical address space upto (2^64). So page table can become excessively large. If page table is large we may not want to keep it continous in memory rather we divide the page into smaller pieces. One way to do so is two level paging algorithm. Because address translation works from the outer page table inward, this scheme is also known as a forward-mapped page table.

A common approach for handling address spaces larger than 32 bits is to use a hashed page table, with the hash value being the virtual page number. Each entry in the hash table contains a linked list of elements that hash to the same location (to handle collisions). Each element consists of three fields: (1) the virtual page number, (2) the value of the mapped page frame, and (3) a pointer to the next element in the linked list.The algorithm works as follows: The virtual page number in the virtual
address is hashed into the hash table. The virtual page number is compared with field 1 in the first element in the linked list. If there is a match, the corresponding page frame (field 2) is used to form the desired physical address.

#### Inverted page table

An inverted page table is a system-wide page table indexed by physical frames that stores (PID, virtual page) mappings, used to reduce memory overhead at the cost of slower address translation.

An **Inverted Page Table (IPT)** is a memory management data structure used by an operating system to map **virtual memory to physical memory**, but **in a reversed way** compared to a normal page table.In a **traditional page table**, **each process** has its own page table, and the table is indexed by **virtual page number (VPN)** to find the corresponding **physical frame number (PFN)**. In contrast, an **inverted page table has only one page table for the entire system**, and it is indexed by **physical frame number**. Each entry tells **which process** and **which virtual page** currently occupies that physical frame.

Each entry in inverted page table has

```
<Processid,virtual page number,control bits>
```

The main reason for using an inverted page table is **memory efficiency**. In normal page table we have one page table per process and with larger byte systems. The page size becomes huge. With the inverted page table we have only one page table. 

How address translation works?

Because the table is indexed by **physical frame number**, we **cannot directly index it using the virtual page number**.
Steps:

1. CPU generates a virtual address → `(PID, VPN, offset)`
2. OS (or hardware) **hashes (PID, VPN)** to find a possible entry
3. Search the inverted page table (often using a **hash table**)
4. If a matching `(PID, VPN)` is found:
    - Physical address = `(frame number, offset)`
5. If not found → **page fault**

Because IPT lookup is expensive, **TLB (Translation Lookaside Buffer)** becomes very important.
- TLB caches recent `(PID, VPN) → PFN` mappings
- Most memory accesses hit in TLB
- IPT lookup is needed only on **TLB miss**
Without a TLB, IPT would be very slow.

## Virtual memory

Virtual memory involves the separation of logical memory as perceived by users from physical memory. This separation allows an extremely large virtual memory to be provided for programmers when only a smaller physical memory is available.
Usual technique here is to not have the entire memory of process loaded into the memory for execution. Rather only partial memory of a process is loaded into main memory. The first advantage is that the total memory needed by the program can be larger than the physical memory available. This allows higher degree of multiprogramming.

### Demand paging

Demand paging is an approach to implement virtual memory. Pages are loaded into the memory only when the CPU wants to access them. Pages that the CPU does not want to access are not loaded. We use a program called lazy swapper; also known as pager. Note how it differs from the technique we have assumed till now that entire process is loaded first into the memory. 

If the CPU wants to access a memory-resident page, then the execution continues normally. If the CPU wants to access a page that is not in memory, then a page fault trap occurs. (trap: highest priority non-maskable external-hardware interrupt)
Pager or swapper instead of swapping in a whole process, the pager brings only those pages into memory. Thus, it avoids reading into memory pages that will not be used anyway, decreasing the swap time and the amount of physical memory needed.

![Pasted image 20260112102252.png](/notes-images/Pasted%20image%2020260112102252.png)

In pure demand paging the proces starts with no page in memory the start of process is fast but the response time anyway is low.

I p is the page fault ratio then -

```
Effective time = p*(page fault service time)+(1-p)*memory access time
```

Page replacement is the process of replacing one page by another in the memory when there is no
free frame.
How to load a page in memory when there is no free frame?
1. Save the content of the page currently in memory on disk.
2. Load new page.
3. Update page table.
Avoid saving unmodified pages –
- use modify bit / dirty bit

NOw for a page to be replaced we add a modified/dirty bit ot the page table. If the page was modified then dirty bit is set. When swapping out this page if modified bit is set. Then this page will be written back. Otherwise page will not be written back to the disk. 

### Page replacement algorithm

#### FIFO page replacement

The simplest page-replacement algorithm is a first-in, first-out (FIFO) algorithm. A FIFO replacement algorithm associates with each page the time when that page was brought into memory. When a page must be replaced, the oldest page is chosen.

On the one hand, the page replaced may be an initialization module that was used a long time ago and is no longer needed. On the other hand, it could contain a heavily used variable that was initialized early and is in constant use. Any we may repalce that and again when needed we may need to bring it back. 

In the usual case the higher number of frames available lesser will be the page fault for a given algorthim. This makes sence as we have larger space now. Now if an algorithm can in even one single case have larger page fault with larger number of frames available then this algorithm is said to be suffering from `Belady's anomaly`. FIFO page replacement suffers from belady's anomaly. 

#### Optimal page replacement

Replace the page that will not be used for the longest period of time.
This is most optimal and does not suffers from belady's anomaly. But it is impossible to implement. 

#### Least-recently used(LRU)

Replace the least recently used page. This does not have belady anomay. 

Implementing LRU replacement is to keep a stack of page numbers. Whenever a page is referenced, it is removed
from the stack and put on the top. In this way, the most recently used page is always at the top of the stack and the least recently used page is always at the bottom (Figure 9.16). Because entries must be removed from the middle of the stack, it is best to implement this approach by using a doubly linked list with a head pointer and a tail pointer. Removing a page
and putting it on the top of the stack then requires changing six pointers at worst. Each update is a little more expensive, but there is no search for a replacement; the tail pointer points to the bottom of the stack, which is the LRU page. This approach is particularly appropriate for software or microcode implementations of LRU replacement. LRU can not be implemented without hardware assitance reason being that we need to use interupt for each memory access. But interupt will make it slow. 

However if hardware assistance is provided which can do all these tasks then we can implement LRU. If hardware assitance is not provided we can use LRU approximation. 

### Threashing

Thrashing is a situation when a system is spending more time in servicing page faults than executing. If a process does not have the number of frames it needs to store the pages in active use, then it will page fault frequently. Since all the pages in memory are in active use, the page that will be replaced will be needed again. This will bring down the CPU utilization and the system will load more processes in the memory in order to increase the degree of multiprogramming.
This will trigger a chain reaction of page faults. This situation when the CPU utilization diminishes due to high paging activity is called thrashing.

